{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiL00nYLllKG"
      },
      "source": [
        "# Face Detection on Custom Dataset with Detectron2\n",
        "\n",
        "Face detection is the task of finding (boundaries of) faces in images. This is useful for\n",
        "- security systems (the first step in recognizing a person)\n",
        "- autofocus and smile detection for making great photos\n",
        "- detecting age, race, and emotional state for markering (yep, we already live in that world)\n",
        "\n",
        "Historically, this was a really tough problem to solve. Tons of manual feature engineering, novel algorithms and methods were developed to improve the state-of-the-art.\n",
        "\n",
        "These days, face detection models are included in almost every computer vision package/framework. Some of the best-performing ones use Deep Learning methods. OpenCV, for example, provides a variety of tools like the [Cascade Classifier](https://docs.opencv.org/4.2.0/db/d28/tutorial_cascade_classifier.html).\n",
        "\n",
        "- [Run the complete notebook in your browser (Google Colab)](https://colab.research.google.com/drive/1Jk4-qX9zdYGsBrTnh2vF52CV9ucuqpjk)\n",
        "- [Read the **Getting Things Done with Pytorch** book](https://github.com/curiousily/Getting-Things-Done-with-Pytorch)\n",
        "\n",
        "In this guide, you'll learn how to:\n",
        "- prepare a custom dataset for face detection with Detectron2\n",
        "- use (close to) state-of-the-art models for object detection to find faces in images\n",
        "- You can extend this work for face recognition.\n",
        "\n",
        "Here's an example of what you'll get at the end of this guide:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCJr8jdm2xGK"
      },
      "source": [
        "<img src='https://www.curiousily.com/media/pytorch-02/social-image.png' />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf6nGij52aXT"
      },
      "source": [
        "## Detectron 2\n",
        "\n",
        "<img src=\"https://www.curiousily.com/media/pytorch-02/detectron2-logo.png\" width=\"55%\">\n",
        "\n",
        "[Detectron2](https://github.com/facebookresearch/detectron2) is a framework for building state-of-the-art object detection and image segmentation models. It is developed by the Facebook Research team. Detectron2 is a complete rewrite of the [first version](https://github.com/facebookresearch/Detectron).\n",
        "\n",
        "Under the hood, Detectron2 uses PyTorch (compatible with the latest version(s)) and allows for [blazing fast training](https://detectron2.readthedocs.io/notes/benchmarks.html). You can learn more at [introductory blog post](https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/) by Facebook Research.\n",
        "\n",
        "The real power of Detectron2 lies in the HUGE amount of pre-trained models available at the [Model Zoo](https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md). But what good that would it be if you can't fine-tune those on your own datasets? Fortunately, that's super easy! We'll see how it is done in this guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7HyKvbEliZy"
      },
      "source": [
        "### Installing Detectron2\n",
        "\n",
        "At the time of this writing, Detectron2 is still in an alpha stage. While there is an official release, we'll clone and compile from the master branch. This should equal version 0.1.\n",
        "\n",
        "Let's start by installing some requirements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wauAzIX65I6_",
        "outputId": "09127d44-5a65-429c-aa58-ee9d2c294a46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -q cython pyyaml==6.0.1\n",
        "!pip install -q -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S6wIc2Oti_c"
      },
      "source": [
        "And download, compile, and install the Detectron2 package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmev2LhSCWFw",
        "outputId": "95568a77-beec-4d52-95b9-612614d5478f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-kn57uboe\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-kn57uboe\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit 92ae9f0b92aba5867824b4f12aa06a22a60a45d3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.7.1)\n",
            "Collecting pycocotools>=2.0.2 (from detectron2==0.6)\n",
            "  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.4.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.66.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.15.2)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting omegaconf<2.4,>=2.1 (from detectron2==0.6)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting black (from detectron2==0.6)\n",
            "  Downloading black-24.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (24.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.25.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.1)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.11.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.62.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6) (3.2.2)\n",
            "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp310-cp310-linux_x86_64.whl size=6147824 sha256=129d170be6ab17f87ab3d8eab2010c70b63bc9470e7efefa4410dea7883d7779\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l19qudjy/wheels/47/e5/15/94c80df2ba85500c5d76599cc307c0a7079d0e221bb6fc4375\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=b702487e086bd55eb6e7c418906565e75650cc470209998508c3495da04b39ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=2b08268cc5e284e296d9156daf5068f3d246ac82435bac50d492b3782832f38d\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built detectron2 fvcore antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, pycocotools, fvcore, detectron2\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0\n",
            "    Uninstalling pycocotools-2.0:\n",
            "      Successfully uninstalled pycocotools-2.0\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-24.4.2 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.12.1 portalocker-2.8.2 pycocotools-2.0.7 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVGrjW_Gt4Gq"
      },
      "source": [
        "> At this point, you'll need to restart the notebook runtime to continue!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTGoAFdXeg1I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea53b77f-6924-4aa8-8d05-82b3afa518b2"
      },
      "source": [
        "!pip install -q -U watermark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.6 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,pycocotools,torch,torchvision,detectron2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7d-kQV8vkgz",
        "outputId": "c619c7bf-e018-4b29-a4eb-1abfd8d7bc34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python implementation: CPython\n",
            "Python version       : 3.10.12\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "numpy      : 1.25.2\n",
            "pandas     : 2.0.3\n",
            "pycocotools: 2.0.7\n",
            "torch      : 2.2.1+cu121\n",
            "torchvision: 0.17.1+cu121\n",
            "detectron2 : 0.6\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMHd00OAt11C",
        "outputId": "7d591908-8db0-4db9-abc1-0f62b8146913",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch, torchvision\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "import glob\n",
        "\n",
        "import os\n",
        "import ntpath\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import urllib\n",
        "import json\n",
        "import PIL.Image as Image\n",
        "\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.structures import BoxMode\n",
        "\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78125a9195b0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4XN0qCWz4nL"
      },
      "source": [
        "## Face Detection Data\n",
        "\n",
        "Our dataset is provided by [Dataturks](https://dataturks.com/), and it is hosted on [Kaggle](https://www.kaggle.com/dataturks/face-detection-in-images). Here's an excerpt from the description:\n",
        "\n",
        "> Faces in images marked with bounding boxes. Have around 500 images with around 1100 faces manually tagged via bounding box.\n",
        "\n",
        "I've downloaded the JSON file containing the annotations and uploaded it to Google Drive. Let's get it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz4pEVRjT_64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c361d8-88ef-4a15-ceeb-9ac4de6c7b4a"
      },
      "source": [
        "import gdown\n",
        "!gdown --id 1G5bAb1iMWX3_Zb42WgIkrTBJEXofhEJl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1G5bAb1iMWX3_Zb42WgIkrTBJEXofhEJl\n",
            "To: /content/wider_face_train_annot_coco_style.json\n",
            "100% 92.9M/92.9M [00:00<00:00, 140MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSUzacxX2_K1"
      },
      "source": [
        "Let's load the file into a Pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7Bq4GNDzLc1"
      },
      "source": [
        "with open('/content/wider_face_train_annot_coco_style.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "categories = data[\"categories\"]\n",
        "images = data[\"images\"]\n",
        "annotations = data[\"annotations\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories_id = data[\"categories\"][0][\"name\"]\n",
        "print(categories_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mxnz7iUd70XO",
        "outputId": "051e89e4-c135-4432-d816-1fd0f313cc50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "face\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKQeltR927BM"
      },
      "source": [
        "Each line contains a single face annotation. Note that multiple lines might point to a single image (e.g. multiple faces per image)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97eCU2d9SQBB"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "The dataset contains only image URLs and annotations. We'll have to download the images. We'll also normalize the annotations, so it's easier to use them with Detectron2 later on:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_annotation(annotation):\n",
        "  # Extract bounding box data\n",
        "  bbox = annotation[\"bbox\"]\n",
        "\n",
        "  # Bounding box coordinates\n",
        "  x_min = bbox[0]\n",
        "  y_min = bbox[1]\n",
        "  width = bbox[2]\n",
        "  height = bbox[3]\n",
        "\n",
        "  # Bounding box area\n",
        "  bbox_area = annotation[\"area\"]\n",
        "\n",
        "  if bbox_area == 0:\n",
        "      return None\n",
        "\n",
        "  # Calculate the maximum and minimum values for normalization\n",
        "  max_value = max(x_min, y_min, x_min + width, y_min + height)\n",
        "  min_value = min(x_min, y_min, x_min + width, y_min + height)\n",
        "  range_value = max_value - min_value\n",
        "\n",
        "  # Normalize the coordinates using min-max normalization\n",
        "  x_min_norm = (x_min - min_value) / range_value\n",
        "  y_min_norm = (y_min - min_value) / range_value\n",
        "  x_max_norm = ((x_min + width) - min_value) / range_value\n",
        "  y_max_norm = ((y_min + height) - min_value) / range_value\n",
        "\n",
        "  return {\n",
        "      \"file_name\": annotation[\"image_id\"],\n",
        "      \"bbox_area\": bbox_area,\n",
        "      \"x_min\": x_min_norm,\n",
        "      \"y_min\": y_min_norm,\n",
        "      \"x_max\": x_max_norm,\n",
        "      \"y_max\": y_max_norm,\n",
        "      \"class_name\": \"face\",\n",
        "  }\n",
        "\n",
        "def preprocess_and_save(annotations, image_directory):\n",
        "    dataset = []\n",
        "\n",
        "    for annotation in annotations:\n",
        "        processed_data = process_annotation(annotation)\n",
        "        if processed_data:\n",
        "            dataset.append(processed_data)\n",
        "\n",
        "            # Save the corresponding image\n",
        "            file_name = processed_data[\"file_name\"]\n",
        "            file_path = f'{image_directory}/{file_name}'\n",
        "            if not os.path.exists(file_path):\n",
        "                # Code to save or copy image to this path\n",
        "                # Ensure `faces/` contains necessary images or adapt this part\n",
        "                pass\n",
        "\n",
        "    # Convert the dataset to a DataFrame\n",
        "    df = pd.DataFrame(dataset)\n",
        "\n",
        "    print(df.head())\n",
        "    df.to_csv('annotations.csv', header=True, index=None)"
      ],
      "metadata": {
        "id": "c_COM1U78GlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_and_save(annotations, \"faces\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk4cGkNz-t5d",
        "outputId": "eb09e219-a3df-4fb0-9da2-c9c080b416eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   file_name  bbox_area     x_min     y_min     x_max     y_max class_name\n",
            "0          0    18178.0  0.493776  0.000000  1.000000  0.618257       face\n",
            "1          1    89157.0  0.500000  0.000000  1.000000  0.644487       face\n",
            "2          2       56.0  0.000000  0.947020  0.046358  1.000000       face\n",
            "3          2      238.0  0.000000  0.903955  0.079096  1.000000       face\n",
            "4          2      165.0  0.000000  0.868421  0.096491  1.000000       face\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQoBe8RxhUQU"
      },
      "source": [
        "Let's put the data into a dataframe so we can have a better look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih4HkzfNUpsd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "85bbd48f-2fcf-4a09-a781-9823aa2c5327"
      },
      "source": [
        "df = pd.DataFrame(dataset)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   file_name  bbox_area     x_min     y_min     x_max     y_max class_name\n",
              "0          0    18178.0  0.493776  0.000000  1.000000  0.618257       face\n",
              "1          1    89157.0  0.500000  0.000000  1.000000  0.644487       face\n",
              "2          2       56.0  0.000000  0.947020  0.046358  1.000000       face\n",
              "3          2      238.0  0.000000  0.903955  0.079096  1.000000       face\n",
              "4          2      165.0  0.000000  0.868421  0.096491  1.000000       face"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bb27fc6f-4773-4f8c-8d5a-fa8613ad255e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>bbox_area</th>\n",
              "      <th>x_min</th>\n",
              "      <th>y_min</th>\n",
              "      <th>x_max</th>\n",
              "      <th>y_max</th>\n",
              "      <th>class_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>18178.0</td>\n",
              "      <td>0.493776</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.618257</td>\n",
              "      <td>face</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>89157.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.644487</td>\n",
              "      <td>face</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.947020</td>\n",
              "      <td>0.046358</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>face</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>238.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.903955</td>\n",
              "      <td>0.079096</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>face</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>165.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.868421</td>\n",
              "      <td>0.096491</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>face</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb27fc6f-4773-4f8c-8d5a-fa8613ad255e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bb27fc6f-4773-4f8c-8d5a-fa8613ad255e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bb27fc6f-4773-4f8c-8d5a-fa8613ad255e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4d2ef1b7-bd3e-486d-89be-a4bced883dde\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4d2ef1b7-bd3e-486d-89be-a4bced883dde')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4d2ef1b7-bd3e-486d-89be-a4bced883dde button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_XoG7oSUrT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da664e4-aaff-4fe6-eb9f-a22aeb2f4bca"
      },
      "source": [
        "print(df.file_name.unique().shape[0], df.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12876 159393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXaR93rbi8Pz"
      },
      "source": [
        "We have a total of 409 images (a lot less than the promised 500) and 1132 annotations. Let's save them to the disk (so you might reuse them):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTnG9MkHjnKd"
      },
      "source": [
        "### Data Exploration\n",
        "\n",
        "Let's see some sample annotated data. We'll use OpenCV to load an image, add the bounding boxes, and resize it. We'll define a helper function to do it all:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zwCL9lsSAda"
      },
      "source": [
        "def annotate_image(annotations, resize=True):\n",
        "    if annotations.empty:\n",
        "        print(\"No annotations found for this image.\")\n",
        "        return None\n",
        "\n",
        "    file_name = annotations['file_name'].to_numpy()[0]\n",
        "    file_path = f'faces/{file_name}'\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return None\n",
        "\n",
        "    img = cv2.cvtColor(cv2.imread(file_path), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    for i, a in annotations.iterrows():\n",
        "        cv2.rectangle(img, (a.x_min, a.y_min), (a.x_max, a.y_max), (0, 255, 0), 2)\n",
        "\n",
        "    if not resize:\n",
        "        return img\n",
        "\n",
        "    return cv2.resize(img, (384, 384), interpolation=cv2.INTER_AREA)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWAvaiINk1w-"
      },
      "source": [
        "Let's start by showing some annotated images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80cUVP83k2o3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f080733-ea20-4a35-b4aa-2727577dcf27"
      },
      "source": [
        "unique_files = df['file_name'].unique()\n",
        "\n",
        "for file in unique_files[:5]:  # Display the first 5 images\n",
        "    img_df = df[df['file_name'] == file]\n",
        "\n",
        "    if img_df.empty:\n",
        "        print(f\"No annotations found for file: {file}\")\n",
        "        continue\n",
        "\n",
        "    annotated_img = annotate_image(img_df)\n",
        "\n",
        "    if annotated_img is not None:\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.imshow(annotated_img)\n",
        "        plt.title(f\"Annotations for {file}\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File not found: faces/0\n",
            "File not found: faces/1\n",
            "File not found: faces/2\n",
            "File not found: faces/3\n",
            "File not found: faces/4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhJNfLcclKTW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "34cced21-6f11-4501-d837-c38daceb3ab2"
      },
      "source": [
        "img_df = df[df.file_name == df.file_name.unique()[1]]\n",
        "img = annotate_image(img_df, resize=False)\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Image file faces/1 does not exist.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Image data of dtype object cannot be converted to float",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-bde898b76f5d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotate_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0minterpolation_stage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         resample=None, url=None, data=None, **kwargs):\n\u001b[0;32m-> 2695\u001b[0;31m     __ret = gca().imshow(\n\u001b[0m\u001b[1;32m   2696\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5663\u001b[0m                               **kwargs)\n\u001b[1;32m   5664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5665\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5666\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5667\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    699\u001b[0m         if (self._A.dtype != np.uint8 and\n\u001b[1;32m    700\u001b[0m                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n\u001b[0;32m--> 701\u001b[0;31m             raise TypeError(\"Image data of dtype {} cannot be converted to \"\n\u001b[0m\u001b[1;32m    702\u001b[0m                             \"float\".format(self._A.dtype))\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 692,
              "height": 674
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "303IKbrQOSjw"
      },
      "source": [
        "Those are good ones, the annotations are clearly visible. We can use torchvision to create a grid of images. Note that the images are in various sizes, so we'll resize them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfATJdCiXXBH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "ec2eaf12-94ff-4dac-c0e1-592aaeaa8fc6"
      },
      "source": [
        "sample_images = [annotate_image(df[df.file_name == f]) for f in df.file_name.unique()[:10]]\n",
        "sample_images = torch.as_tensor(sample_images)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-3b0c6cea6297>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mannotate_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msample_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-3b0c6cea6297>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mannotate_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msample_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-f6aff633f9f3>\u001b[0m in \u001b[0;36mannotate_image\u001b[0;34m(annotations, resize)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mannotate_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'faces/{file_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wNqZxG-KCKy"
      },
      "source": [
        "sample_images.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqCi0xt5G5pS"
      },
      "source": [
        "sample_images = sample_images.permute(0, 3, 1, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qFznK8sKFSB"
      },
      "source": [
        "sample_images.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc2UEpTbASzD"
      },
      "source": [
        "plt.figure(figsize=(24, 12))\n",
        "grid_img = torchvision.utils.make_grid(sample_images, nrow=5)\n",
        "\n",
        "plt.imshow(grid_img.permute(1, 2, 0))\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxRyNq-5O19z"
      },
      "source": [
        "You can clearly see that some annotations are missing (column 4). That's real life data for you, sometimes you have to deal with it in some way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aG0Gb1xR2zv"
      },
      "source": [
        "## Face Detection with Detectron 2\n",
        "\n",
        "It is time to go through the steps of fine-tuning a model using a custom dataset. But first, let's save 5% of the data for testing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0tuPADAw69d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "2b539d4b-d750-4dd4-a280-6057c3052d99"
      },
      "source": [
        "df = pd.read_csv('annotations.csv')\n",
        "\n",
        "IMAGES_PATH = f'faces'\n",
        "\n",
        "unique_files = df.file_name.unique()\n",
        "\n",
        "train_files = set(np.random.choice(unique_files, int(len(unique_files) * 0.95), replace=False))\n",
        "train_df = df[df.file_name.isin(train_files)]\n",
        "test_df = df[~df.file_name.isin(train_files)]\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   file_name  bbox_area     x_min     y_min     x_max     y_max class_name\n",
              "0          0    18178.0  0.493776  0.000000  1.000000  0.618257       face\n",
              "1          1    89157.0  0.500000  0.000000  1.000000  0.644487       face\n",
              "2          2       56.0  0.000000  0.947020  0.046358  1.000000       face\n",
              "3          2      238.0  0.000000  0.903955  0.079096  1.000000       face\n",
              "4          2      165.0  0.000000  0.868421  0.096491  1.000000       face"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e23e7736-53c5-452d-9e98-ced1757cd470\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>bbox_area</th>\n",
              "      <th>x_min</th>\n",
              "      <th>y_min</th>\n",
              "      <th>x_max</th>\n",
              "      <th>y_max</th>\n",
              "      <th>class_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>18178.0</td>\n",
              "      <td>0.493776</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.618257</td>\n",
              "      <td>face</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>89157.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.644487</td>\n",
              "      <td>face</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.947020</td>\n",
              "      <td>0.046358</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>face</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>238.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.903955</td>\n",
              "      <td>0.079096</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>face</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>165.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.868421</td>\n",
              "      <td>0.096491</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>face</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e23e7736-53c5-452d-9e98-ced1757cd470')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e23e7736-53c5-452d-9e98-ced1757cd470 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e23e7736-53c5-452d-9e98-ced1757cd470');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-93b56536-64e8-4731-864c-edc58dec6b37\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-93b56536-64e8-4731-864c-edc58dec6b37')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-93b56536-64e8-4731-864c-edc58dec6b37 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCwk_UeeSwu2"
      },
      "source": [
        "The classical train_test_split won't work here, cause we want a split amongst the file names.\n",
        "\n",
        "The next parts are written in a bit more generic way. Obviously, we have a single class - face. But adding more should be as simple as adding more annotations to the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQDdB3yHxvm7"
      },
      "source": [
        "classes = df.class_name.unique().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHkbICHfS_LI"
      },
      "source": [
        "Next, we'll write a function that converts our dataset into a format that is used by Detectron2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh6stJEoeTbG"
      },
      "source": [
        "def create_dataset_dicts(df, classes):\n",
        "  dataset_dicts = []\n",
        "  for image_id, img_name in enumerate(df.file_name.unique()):\n",
        "\n",
        "    record = {}\n",
        "\n",
        "    image_df = df[df.file_name == img_name]\n",
        "\n",
        "    file_path = f'{IMAGES_PATH}/{img_name}'\n",
        "    record[\"file_name\"] = file_path\n",
        "    record[\"image_id\"] = image_id\n",
        "    record[\"height\"] = int(image_df.iloc[0].height)\n",
        "    record[\"width\"] = int(image_df.iloc[0].width)\n",
        "\n",
        "    objs = []\n",
        "    for _, row in image_df.iterrows():\n",
        "\n",
        "      xmin = int(row.x_min)\n",
        "      ymin = int(row.y_min)\n",
        "      xmax = int(row.x_max)\n",
        "      ymax = int(row.y_max)\n",
        "\n",
        "      poly = [\n",
        "          (xmin, ymin), (xmax, ymin),\n",
        "          (xmax, ymax), (xmin, ymax)\n",
        "      ]\n",
        "      poly = list(itertools.chain.from_iterable(poly))\n",
        "\n",
        "      obj = {\n",
        "        \"bbox\": [xmin, ymin, xmax, ymax],\n",
        "        \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "        \"segmentation\": [poly],\n",
        "        \"category_id\": classes.index(row.class_name),\n",
        "        \"iscrowd\": 0\n",
        "      }\n",
        "      objs.append(obj)\n",
        "\n",
        "    record[\"annotations\"] = objs\n",
        "    dataset_dicts.append(record)\n",
        "  return dataset_dicts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saOEmpLqW1tK"
      },
      "source": [
        "We convert every annotation row to a single record with a list of annotations. You might also notice that we're building a polygon that is of the exact same shape as the bounding box. This is required for the image segmentation models in Detectron2.\n",
        "\n",
        "You'll have to register your dataset into the dataset and metadata catalogues:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjhVau86xMCa"
      },
      "source": [
        "for d in [\"train\", \"val\"]:\n",
        "  DatasetCatalog.register(\"faces_\" + d, lambda d=d: create_dataset_dicts(train_df if d == \"train\" else test_df, classes))\n",
        "  MetadataCatalog.get(\"faces_\" + d).set(thing_classes=classes)\n",
        "\n",
        "statement_metadata = MetadataCatalog.get(\"faces_train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JDZvlcyZdMf"
      },
      "source": [
        "Unfortunately, evaluator for the test set is not included by default. We can easily fix that by writing our own trainer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UsgI41N9xHa"
      },
      "source": [
        "class CocoTrainer(DefaultTrainer):\n",
        "\n",
        "  @classmethod\n",
        "  def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "\n",
        "    if output_folder is None:\n",
        "        os.makedirs(\"coco_eval\", exist_ok=True)\n",
        "        output_folder = \"coco_eval\"\n",
        "\n",
        "    return COCOEvaluator(dataset_name, cfg, False, output_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYyKENuZZm5v"
      },
      "source": [
        "The evaluation results will be stored in the `coco_eval` folder if no folder is provided.\n",
        "\n",
        "Fine-tuning a Detectron2 model is nothing like writing PyTorch code. We'll load a configuration file, change a few values, and start the training process. But hey, it really helps if you know what you're doing 😂\n",
        "\n",
        "For this tutorial, we'll use the Mask R-CNN X101-FPN model. It is pre-trained on the [COCO dataset](http://cocodataset.org/#home) and achieves very good performance. The downside is that it is slow to train.\n",
        "\n",
        "Let's load the config file and the pre-trained model weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIib8zgaf1sl"
      },
      "source": [
        "cfg = get_cfg()\n",
        "\n",
        "cfg.merge_from_file(\n",
        "  model_zoo.get_config_file(\n",
        "    \"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"\n",
        "  )\n",
        ")\n",
        "\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
        "  \"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sertwKl-iqD2"
      },
      "source": [
        "Specify the datasets (we registered those) we'll use for training and evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmIU6TvKgWfe"
      },
      "source": [
        "cfg.DATASETS.TRAIN = (\"faces_train\",)\n",
        "cfg.DATASETS.TEST = (\"faces_val\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQUD5LutjFxf"
      },
      "source": [
        "And for the optimizer, we'll do a bit of magic to converge to something nice:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnQDunrbgYMX"
      },
      "source": [
        "cfg.SOLVER.IMS_PER_BATCH = 4\n",
        "cfg.SOLVER.BASE_LR = 0.001\n",
        "cfg.SOLVER.WARMUP_ITERS = 1000\n",
        "cfg.SOLVER.MAX_ITER = 1500\n",
        "cfg.SOLVER.STEPS = (1000, 1500)\n",
        "cfg.SOLVER.GAMMA = 0.05"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5xcfeR_jOyj"
      },
      "source": [
        "Except for the standard stuff (batch size, max number of iterations, and learning rate) we have a couple of interesting params:\n",
        "\n",
        "- `WARMUP_ITERS` - the learning rate starts from 0 and goes to the preset one for this number of iterations\n",
        "- `STEPS` - the checkpoints (number of iterations) at which the learning rate will be reduced by `GAMMA`\n",
        "\n",
        "Finally, we'll specify the number of classes and the period at which we'll evaluate on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJhCk3c2gaod"
      },
      "source": [
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(classes)\n",
        "\n",
        "cfg.TEST.EVAL_PERIOD = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX-nqwfak0in"
      },
      "source": [
        "Time to train, using our custom trainer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm-1luynzTiD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "8d132d04-0e98-4fcd-f68f-7b41a46edc3d"
      },
      "source": [
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "trainer = CocoTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-e92df355542f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCocoTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_or_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# Assume these objects must be constructed in this order.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_train_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(cls, cfg)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mOverwrite\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0myou\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0md\u001b[0m \u001b[0mlike\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdifferent\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \"\"\"\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m         \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model:\\n{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/detectron2/modeling/meta_arch/build.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmeta_arch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMETA_ARCHITECTURE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMETA_ARCH_REGISTRY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_arch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0m_log_api_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"modeling.meta_arch.\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmeta_arch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1149\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1150\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRaCjOJADyeD"
      },
      "source": [
        "## Evaluating Object Detection Models\n",
        "\n",
        "Evaluating object detection models is a bit different when compared to evaluating standard classification or regression models.\n",
        "\n",
        "The main metric you need to know about is IoU (intersection over union). It measures the overlap between two boundaries - the predicted and ground truth one. It can get values between 0 and 1.\n",
        "\n",
        "$$\\text{IoU}=\\frac{\\text{area of overlap}}{\\text{area of union}}$$\n",
        "\n",
        "Using IoU, one can define a threshold (e.g. >0.5) to classify whether a prediction is a true positive (TP) or a false positive (FP).\n",
        "\n",
        "Now you can calculate average precision (AP) by taking the area under the precision-recall curve.\n",
        "\n",
        "Now AP@X (e.g. AP50) is just AP at some IoU threshold. This should give you a working understanding of how object detection models are evaluated.\n",
        "\n",
        "I suggest you read the [mAP (mean Average Precision) for Object Detection](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173) tutorial by Jonathan Hui if you want to learn more on the topic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1kBIdhDd8uT"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj9aucZaVFHS"
      },
      "source": [
        "%tensorboard --logdir output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_awe5wYmVZP"
      },
      "source": [
        "I've prepared a pre-trained model for you, so you don't have to wait for the training to complete. Let's download it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7s8Uwz6cb-t"
      },
      "source": [
        "!gdown --id 18Ev2bpdKsBaDufhVKf0cT6RmM3FjW3nL\n",
        "!mv face_detector.pth output/model_final.pth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD4WAmVEnLZn"
      },
      "source": [
        "We can start making predictions by loading the model and setting a minimum threshold of 85% certainty at which we'll consider the predictions as correct:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-VPvJJxzWtM"
      },
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.85\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4SRtxeynUaK"
      },
      "source": [
        "Let's run the evaluator with the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzLLfo90eI_t"
      },
      "source": [
        "evaluator = COCOEvaluator(\"faces_val\", cfg, False, output_dir=\"./output/\")\n",
        "val_loader = build_detection_test_loader(cfg, \"faces_val\")\n",
        "inference_on_dataset(trainer.model, val_loader, evaluator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3BUEjZ-nuvX"
      },
      "source": [
        "### Finding Faces in Images\n",
        "\n",
        "Next, let's create a folder and save all images with predicted annotations in the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsXDCLuxH34u"
      },
      "source": [
        "os.makedirs(\"annotated_results\", exist_ok=True)\n",
        "\n",
        "test_image_paths = test_df.file_name.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0O32pFpHbBh"
      },
      "source": [
        "for clothing_image in test_image_paths:\n",
        "  file_path = f'{IMAGES_PATH}/{clothing_image}'\n",
        "  im = cv2.imread(file_path)\n",
        "  outputs = predictor(im)\n",
        "  v = Visualizer(\n",
        "    im[:, :, ::-1],\n",
        "    metadata=statement_metadata,\n",
        "    scale=1.,\n",
        "    instance_mode=ColorMode.IMAGE\n",
        "  )\n",
        "  instances = outputs[\"instances\"].to(\"cpu\")\n",
        "  instances.remove('pred_masks')\n",
        "  v = v.draw_instance_predictions(instances)\n",
        "  result = v.get_image()[:, :, ::-1]\n",
        "  file_name = ntpath.basename(clothing_image)\n",
        "  write_res = cv2.imwrite(f'annotated_results/{file_name}', result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzEfY1P7n0r-"
      },
      "source": [
        "Let's have a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY7qtFf4ez3e"
      },
      "source": [
        "annotated_images = [f'annotated_results/{f}' for f in test_df.file_name.unique()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUMAe0CbeAbm"
      },
      "source": [
        "img = cv2.cvtColor(cv2.imread(annotated_images[0]), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a324bTYae1nA"
      },
      "source": [
        "img = cv2.cvtColor(cv2.imread(annotated_images[1]), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RguwQjKcfC6K"
      },
      "source": [
        "img = cv2.cvtColor(cv2.imread(annotated_images[3]), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSCw_Uehf89k"
      },
      "source": [
        "img = cv2.cvtColor(cv2.imread(annotated_images[4]), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wigVQpmAoMvw"
      },
      "source": [
        "Not bad. Not bad at all. I suggest you explore more images on your own, too!\n",
        "\n",
        "Note that some faces have multiple bounding boxes (on the second image) with different degrees of certainty. Maybe training the model longer will help? How about adding more or augmenting the existing data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VazlZcDu2VK-"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Congratulations! You now know the basics of Detectron2 for object detection! You might be surprised by the results, given the small dataset we have. That's the power of large pre-trained models for you 😍\n",
        "\n",
        "- [Run the complete notebook in your browser (Google Colab)](https://colab.research.google.com/drive/1Jk4-qX9zdYGsBrTnh2vF52CV9ucuqpjk)\n",
        "- [Read the **Getting Things Done with Pytorch** book](https://github.com/curiousily/Getting-Things-Done-with-Pytorch)\n",
        "\n",
        "You learned how to:\n",
        "- prepare a custom dataset for face detection with Detectron2\n",
        "- use (close to) state-of-the-art models for object detection to find faces in images\n",
        "- You can extend this work for face recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PIifDMYX0Lz"
      },
      "source": [
        "## References\n",
        "\n",
        "- [Face Detection in Images](https://www.kaggle.com/dataturks/face-detection-in-images)\n",
        "- [Detectron2 on GitHub](https://github.com/facebookresearch/detectron2/)\n",
        "- [mAP (mean Average Precision) for Object Detection](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173)"
      ]
    }
  ]
}